{
    "docs": [
        {
            "location": "/", 
            "text": "Getting Started\n#\n\n\n\n\nTip\n\n\n\n\nTo open this lab guide in a separate browser window\n (Right Click \n Open In a New Tab) \nuse \nThis Link\n.\n\n\n\n\n\n\nTest Drive Prerequisites\n#\n\n\nFor this lab you need the following:\n\n\n\n\nWorkstation with Internet access\n\n\nSSH client program installed on your workstation\n\n\n\n\nStarting the LAB\n#\n\n\nAfter you have logged into \nRed Hat Test Drive Portal\n , select \nCeph Data Show : All-in-One\n Test Drive from the catalog Icon.\n\n\n\n\nTo start the Test Drive press \n button in the top bar.\n\n\nBy the time your lab environment is getting ready, you could setup Access Keys as shown below.\n\n\n\n\nAccessing the LAB\n#\n\n\n\n\nImmediately after pressing the \nSTART\n button button, the download links for the SSH keys will become active (blue) in the left hand pane in a section labeled \nConnection Details\n:\n\n\n\n\n\n\n\n\n\n\nDownload the PEM key to your computer if you are using regular OpenSSH on the command line with Linux or macOS. Choose Download PPK if you are using \nPuTTY on Windows\n.\n\n\n\n\n\n\nChange permission of the downloaded SSH private key\n\n\n\n\n\n\nchmod 400 ~/Downloads/qwikLABS*.p*\n\n\n\n\nWait for lab provisioning to complete\n#\n\n\nGenerally it takes less than 15 minutes to provision your lab environment. Once your lab environment is provisioned, you will find login details on the left hand pane in a section labeled \nConnection Details\n \n\n\n\n\nCeph Node SSH IP\n\n\nCeph Node SSH user name\n\n\nCeph Node SSH Password\n\n\nCeph S3 Endpoint\n\n\nOpenShift Web Console URL\n\n\nOpenShift Web Console user name\n\n\nOpenShift Web Console Password\n\n\nOpenShift Master Node SSH IP\n\n\nOpenShift Master Node SSH user name", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Tip   To open this lab guide in a separate browser window  (Right Click   Open In a New Tab)  use  This Link .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#test-drive-prerequisites", 
            "text": "For this lab you need the following:   Workstation with Internet access  SSH client program installed on your workstation", 
            "title": "Test Drive Prerequisites"
        }, 
        {
            "location": "/#starting-the-lab", 
            "text": "After you have logged into  Red Hat Test Drive Portal  , select  Ceph Data Show : All-in-One  Test Drive from the catalog Icon.   To start the Test Drive press   button in the top bar.  By the time your lab environment is getting ready, you could setup Access Keys as shown below.", 
            "title": "Starting the LAB"
        }, 
        {
            "location": "/#accessing-the-lab", 
            "text": "Immediately after pressing the  START  button button, the download links for the SSH keys will become active (blue) in the left hand pane in a section labeled  Connection Details :      Download the PEM key to your computer if you are using regular OpenSSH on the command line with Linux or macOS. Choose Download PPK if you are using  PuTTY on Windows .    Change permission of the downloaded SSH private key    chmod 400 ~/Downloads/qwikLABS*.p*", 
            "title": "Accessing the LAB"
        }, 
        {
            "location": "/#wait-for-lab-provisioning-to-complete", 
            "text": "Generally it takes less than 15 minutes to provision your lab environment. Once your lab environment is provisioned, you will find login details on the left hand pane in a section labeled  Connection Details     Ceph Node SSH IP  Ceph Node SSH user name  Ceph Node SSH Password  Ceph S3 Endpoint  OpenShift Web Console URL  OpenShift Web Console user name  OpenShift Web Console Password  OpenShift Master Node SSH IP  OpenShift Master Node SSH user name", 
            "title": "Wait for lab provisioning to complete"
        }, 
        {
            "location": "/Module-1/", 
            "text": "Module - 1 : Ceph cluster \n Object Storage Setup\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module you will be deploying Red Hat Ceph Storage 3 cluster across 3 nodes using Ansible based deployer called \nceph-ansible\n.\n\n\nYou will also learn how to configure object storage for S3 API by setting up Ceph Rados Gateway (RGW)\n\n\n\n\n\n\n\n\nFrom your workstation SSH into \nceph-node1\n with the user name \nstudent\n and password \nRedhat18\n \n(Need Help..Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-node1\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou must run all the commands logged in as user \nstudent\n on the \nceph-node1\n node, unless otherwise specified. \n\n\n\n\n\n\nFast Forward Deployment\n#\n\n\nIn order to save your precious lab time, this section deploys and configures the Ceph Cluster as well as Ceph S3 Object storage in a highly automated way using a all-in-one shell script. \n\n\n\n\nImportant\n\n\n\n\nIf you are using Fast Forward method of deployment, you could skip the next sections labeled as \nManual Deployment\n\n\n\n\n\n\n\n\nTo start \nFast Forward Deployer\n run the following command\n\n\n\n\nsh /home/student/auto-pilot/setup_ceph_cluster_with_rgw.sh\n\n\n\n\n\n\nThe \nFast Forward Deployer\n should take under 10-12 minutes to complete. Once its done, check the status of your Ceph cluster.\n\n\n\n\nsudo ceph -s\n\n\n\n\n[student@ceph-node1 ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-node1(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-node1 ceph-ansible]$\n\n\n\n\nManual Deployment : Setting up environment for ceph-ansible\n#\n\n\n\n\nImportant\n\n\n\n\nIf you have choose to follow the above \nFast Forward Deployment\n method, you should skip the below \nManual Deployment\n process.\n\n\n\n\n\n\n\n\nBegin by creating a directory for ceph-ansible keys under \nstudent\n user\ns home directory.\n\n\n\n\nmkdir ~/ceph-ansible-keys\n\n\n\n\n\n\nCreate a new ansible inventory file which helps \nceph-ansible\n to know what role needs to be applied on each node.\n\n\n\n\nsudo vi /etc/ansible/hosts\n\n\n\n\n\n\nIn the \n/etc/ansible/hosts\n inventory file add the following\n\n\n\n\n[mons]\n\n\nceph-node[1:3]\n\n\n\n[osds]\n\n\nceph-node[1:3]\n\n\n\n[mgrs]\n\n\nceph-node1\n\n\n\n[clients]\n\n\nceph-node1\n\n\n\n[rgws]\n\n\nceph-node1\n\n\n\n\n\n\n\nInfo\n\n\n\n\nSince this is a lab environment we are collocating Ceph Mon and Ceph OSD daemons on \nceph-node*\n nodes\n\n\nAlso \nceph-node1\n node will host Ceph Client, Ceph Manager and Ceph RGW services\n\n\n\n\n\n\n\n\nBefore we begin Ceph deployment, make sure that Ansible can reach to all the cluster nodes.\n\n\n\n\nansible all -m ping\n\n\n\n\nManual Deployment : Configuring Ceph-Ansible Settings\n#\n\n\n\n\nVisit \nceph-ansible\n main configuration directory\n\n\n\n\ncd /usr/share/ceph-ansible/group_vars/\n\n\n\n\n\n\nIn the directory you will find \nall.yml\n , \nosds.yml\n and \nclients.yml\n configuration files which are \npre-populated for you\n to avoid any typographic errors. Lets look at these configuration files one by one.\n\n\n\n\n\n\nTip\n\n\nYou can skip editing configuration files as they are pre-populated with correct settings to avoid typos and save time.\n\n\n\n\n\n\nall.yml\n configuration file most importantly configures\n\n\nCeph repository, path to RHCS ISO\n\n\nCeph Monitor network interface ID, public network\n\n\nCeph OSD backend as \nfilestore\n\n\nCeph RGW port, threads and interface\n\n\nCeph configuration settings for pools\n\n\n\n\n\n\n\n\ncat all.yml\n\n\n\n\n---\n\n\ndummy:\n\n\nfetch_directory: ~/ceph-ansible-keys\n\n\nceph_repository_type: iso\n\n\nceph_origin: repository\n\n\nceph_repository: rhcs\n\n\nceph_rhcs_version: 3\n\n\nceph_rhcs_iso_path: \n/home/student/rhceph-3.0-rhel-7-x86_64.iso\n\n\n\nmonitor_interface: eth0\n\n\nmon_use_fqdn: true\n\n\npublic_network: 10.0.1.0/24\n\n\nosd_objectstore: filestore\n\n\n\nradosgw_civetweb_port: 80\n\n\nradosgw_civetweb_num_threads: 512\n\n\nradosgw_civetweb_options: \nnum_threads=\n{{\n \nradosgw_civetweb_num_threads\n \n}}\n\n\nradosgw_interface: eth0\n\n\nradosgw_dns_name: \nceph-node1\n\n\n\nceph_conf_overrides:\n\n\n  global:\n\n\n    osd pool default pg num: 64\n\n\n    osd pool default pgp num: 64\n\n\n    mon allow pool delete: true\n\n\n    mon clock drift allowed: 5\n\n\n    rgw dns name: \nceph-node1\n\n\n\n\n\n\n\nosds.yml\n configuration file most importantly configures\n\n\nCeph OSD deployment scenario to be collocated (ceph-data and ceph-journal on same device)\n\n\nAuto discover storage device and use them as Ceph OSD\n\n\nAllow Ceph OSD nodes to be ceph admin nodes (optional)\n\n\n\n\n\n\n\n\ncat osds.yml\n\n\n\n\n---\ndummy:\ncopy_admin_key: true\nosd_auto_discovery: true\nosd_scenario: collocated\n\n\n\n\n\n\nclients.yml\n configuration file most importantly configures\n\n\nAllow Ceph client nodes to issue ceph admin commands (optional, not recomended for production)\n\n\n\n\n\n\n\n\ncat clients.yml\n\n\n\n\n---\ndummy:\ncopy_admin_key: True\n\n\n\n\nManual Deployment of RHCS Cluster\n#\n\n\n\n\nTo start deploying RHCS cluster, switch to \nceph-ansible\n root directory\n\n\n\n\ncd /usr/share/ceph-ansible\n\n\n\n\n\n\nRun \nceph-ansible\n playbook\n\n\n\n\ntime ansible-playbook site.yml\n\n\n\n\n\n\nThis should usually take 10-12 minutes to complete. Once its done, play recap should look similar to below. Make sure play recap does not show any host run failed.\n\n\n\n\nPLAY RECAP ******************************************************************\nceph-node1                 : ok=149  changed=29   unreachable=0    failed=0\nceph-node2                 : ok=136  changed=24   unreachable=0    failed=0\nceph-node3                 : ok=138  changed=25   unreachable=0    failed=0\n\nreal  10m9.966s\nuser  2m6.029s\nsys 1m1.005s\n\n\n\n\n\n\nFinally check the status of your cluster. \n\n\n\n\nsudo ceph -s\n\n\n\n\n[student@ceph-node1 ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-node1(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-node1 ceph-ansible]$\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached to the end of Module-1. At this point you should have a healthy RHCS cluster up and running with 3 x Ceph Monitors, 3 x Ceph OSDs (12 x OSDs), 1 x Ceph Manager , 1 x Ceph RGW.", 
            "title": "Module-1 : Ceph cluster & Object Storage Setup"
        }, 
        {
            "location": "/Module-1/#module-1-ceph-cluster-object-storage-setup", 
            "text": "Module Agenda   In this module you will be deploying Red Hat Ceph Storage 3 cluster across 3 nodes using Ansible based deployer called  ceph-ansible .  You will also learn how to configure object storage for S3 API by setting up Ceph Rados Gateway (RGW)     From your workstation SSH into  ceph-node1  with the user name  student  and password  Redhat18   (Need Help..Learn how to Login)   ssh student@ IP Address of ceph-node1    Prerequisite   You must run all the commands logged in as user  student  on the  ceph-node1  node, unless otherwise specified.", 
            "title": "Module - 1 : Ceph cluster &amp; Object Storage Setup"
        }, 
        {
            "location": "/Module-1/#fast-forward-deployment", 
            "text": "In order to save your precious lab time, this section deploys and configures the Ceph Cluster as well as Ceph S3 Object storage in a highly automated way using a all-in-one shell script.    Important   If you are using Fast Forward method of deployment, you could skip the next sections labeled as  Manual Deployment     To start  Fast Forward Deployer  run the following command   sh /home/student/auto-pilot/setup_ceph_cluster_with_rgw.sh   The  Fast Forward Deployer  should take under 10-12 minutes to complete. Once its done, check the status of your Ceph cluster.   sudo ceph -s  [student@ceph-node1 ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-node1(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-node1 ceph-ansible]$", 
            "title": "Fast Forward Deployment"
        }, 
        {
            "location": "/Module-1/#manual-deployment-setting-up-environment-for-ceph-ansible", 
            "text": "Important   If you have choose to follow the above  Fast Forward Deployment  method, you should skip the below  Manual Deployment  process.     Begin by creating a directory for ceph-ansible keys under  student  user s home directory.   mkdir ~/ceph-ansible-keys   Create a new ansible inventory file which helps  ceph-ansible  to know what role needs to be applied on each node.   sudo vi /etc/ansible/hosts   In the  /etc/ansible/hosts  inventory file add the following   [mons]  ceph-node[1:3]  [osds]  ceph-node[1:3]  [mgrs]  ceph-node1  [clients]  ceph-node1  [rgws]  ceph-node1    Info   Since this is a lab environment we are collocating Ceph Mon and Ceph OSD daemons on  ceph-node*  nodes  Also  ceph-node1  node will host Ceph Client, Ceph Manager and Ceph RGW services     Before we begin Ceph deployment, make sure that Ansible can reach to all the cluster nodes.   ansible all -m ping", 
            "title": "Manual Deployment : Setting up environment for ceph-ansible"
        }, 
        {
            "location": "/Module-1/#manual-deployment-configuring-ceph-ansible-settings", 
            "text": "Visit  ceph-ansible  main configuration directory   cd /usr/share/ceph-ansible/group_vars/   In the directory you will find  all.yml  ,  osds.yml  and  clients.yml  configuration files which are  pre-populated for you  to avoid any typographic errors. Lets look at these configuration files one by one.    Tip  You can skip editing configuration files as they are pre-populated with correct settings to avoid typos and save time.    all.yml  configuration file most importantly configures  Ceph repository, path to RHCS ISO  Ceph Monitor network interface ID, public network  Ceph OSD backend as  filestore  Ceph RGW port, threads and interface  Ceph configuration settings for pools     cat all.yml  ---  dummy:  fetch_directory: ~/ceph-ansible-keys  ceph_repository_type: iso  ceph_origin: repository  ceph_repository: rhcs  ceph_rhcs_version: 3  ceph_rhcs_iso_path:  /home/student/rhceph-3.0-rhel-7-x86_64.iso  monitor_interface: eth0  mon_use_fqdn: true  public_network: 10.0.1.0/24  osd_objectstore: filestore  radosgw_civetweb_port: 80  radosgw_civetweb_num_threads: 512  radosgw_civetweb_options:  num_threads= {{   radosgw_civetweb_num_threads   }}  radosgw_interface: eth0  radosgw_dns_name:  ceph-node1  ceph_conf_overrides:    global:      osd pool default pg num: 64      osd pool default pgp num: 64      mon allow pool delete: true      mon clock drift allowed: 5      rgw dns name:  ceph-node1    osds.yml  configuration file most importantly configures  Ceph OSD deployment scenario to be collocated (ceph-data and ceph-journal on same device)  Auto discover storage device and use them as Ceph OSD  Allow Ceph OSD nodes to be ceph admin nodes (optional)     cat osds.yml  ---\ndummy:\ncopy_admin_key: true\nosd_auto_discovery: true\nosd_scenario: collocated   clients.yml  configuration file most importantly configures  Allow Ceph client nodes to issue ceph admin commands (optional, not recomended for production)     cat clients.yml  ---\ndummy:\ncopy_admin_key: True", 
            "title": "Manual Deployment : Configuring Ceph-Ansible Settings"
        }, 
        {
            "location": "/Module-1/#manual-deployment-of-rhcs-cluster", 
            "text": "To start deploying RHCS cluster, switch to  ceph-ansible  root directory   cd /usr/share/ceph-ansible   Run  ceph-ansible  playbook   time ansible-playbook site.yml   This should usually take 10-12 minutes to complete. Once its done, play recap should look similar to below. Make sure play recap does not show any host run failed.   PLAY RECAP ******************************************************************\nceph-node1                 : ok=149  changed=29   unreachable=0    failed=0\nceph-node2                 : ok=136  changed=24   unreachable=0    failed=0\nceph-node3                 : ok=138  changed=25   unreachable=0    failed=0\n\nreal  10m9.966s\nuser  2m6.029s\nsys 1m1.005s   Finally check the status of your cluster.    sudo ceph -s  [student@ceph-node1 ceph-ansible]$ sudo ceph -s\n  cluster:\n    id:     908c17fc-1da0-4569-a25a-f1a23f2e101e\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3\n    mgr: ceph-node1(active)\n    osd: 12 osds: 12 up, 12 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 bytes\n    usage:   1290 MB used, 5935 GB / 5937 GB avail\n    pgs:\n\n[student@ceph-node1 ceph-ansible]$   End of Module  We have reached to the end of Module-1. At this point you should have a healthy RHCS cluster up and running with 3 x Ceph Monitors, 3 x Ceph OSDs (12 x OSDs), 1 x Ceph Manager , 1 x Ceph RGW.", 
            "title": "Manual Deployment of RHCS Cluster"
        }, 
        {
            "location": "/Module-2/", 
            "text": "Module - 2 : Setting up JupyterHub in OpenShift Container Platform\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module you will be creating an OpenShift project for JupyterHub required for the rest of the sessions\n\n\n\n\n\n\n\n\nFrom your workstation SSH into \nOpenShift Master Node\n with the user name \ncloud-user\n and \nSSH Private Key\n \n(Need Help..Learn how to Login)\n\n\n\n\nchmod 400 \npath to ssh_key.pem\n\nssh -i \npath to ssh_key.pem\n cloud-user@\nOpenShift Master Node SSH IP Address\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou must run all the commands logged in as user \ncloud-user\n on the \nOpenShift Master Node\n node, unless otherwise specified. \n\n\n\n\n\n\nCreate an OpenShift Project\n#\n\n\nIn order to save your precious lab time, OpenShift Container Platform has already been installed and configured. Before you begin with some data science exercises, let\ns create an OpenShift project.\n\n\n\n\nSSH into OpenShift Master Node as \ncloud-user\n\n\n\n\nssh -i \npath to ssh_key.pem\n cloud-user@\nOpenShift Master Node IP Address\n\n\n\n\n\n\n\nLogin to OpenShift\n\n\n\n\noc login -u teamuser1 -p openshift\n\n\n\n\n\n\nCreate a new project\n\n\n\n\noc new-project jupyterhub\n\n\n\n\n\n\nPrepare the OpenShift notebooks and templates\n\n\n\n\noc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/notebooks.json\n\n\n\n\noc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/templates.json\n\n\n\n\n\n\nProcess the template to deploy the JupyterHub application\n\n\n\n\noc process jupyterhub-ocp-oauth HASH_AUTHENTICATOR_SECRET_KEY=\nmeh\n | oc apply -f -\n\n\n\n\n\n\nOnce templates are successfully applied, the output should look like this\n\n\n\n\nimagestream \njupyterhub-img\n created\nbuildconfig \njupyterhub-img\n created\nconfigmap \njupyterhub-cfg\n created\nserviceaccount \njupyterhub-hub\n created\nrolebinding \njupyterhub-edit\n created\ndeploymentconfig \njupyterhub\n created\nservice \njupyterhub\n created\nroute \njupyterhub\n created\npersistentvolumeclaim \njupyterhub-db\n created\ndeploymentconfig \njupyterhub-db\n created\nservice \njupyterhub-db\n created\n\n\n\n\n\n\nYou should now have JupyterHub pods and services coming up (will take some time to fully start). \n\n\nTo check the status of pods and services, execute\n\n\n\n\noc get pods\n\n\n\n\n\n\nThe deployment is complete when the jupyterhub-db and jupyter pods are fully running and the output should look like this.\n\n\n\n\n$ oc get pods\nNAME                     READY     STATUS    RESTARTS   AGE\njupyterhub-db-1-deploy   \n1\n/1       Running   \n0\n          19s\njupyterhub-db-1-zkmxc    \n0\n/1       Running   \n0\n          17s\njupyterhub-img-1-build   \n1\n/1       Running   \n0\n          20s\n\n\n\n\n\n\n\n\nYou could also monitor your application from OpenShift Container Platform Console by visiting \nOpenShift Web Console URL\n that you can get from \nQwiklab Portal under Connection Details\n.\n\n\n\n\n\n\nThe user name and password to access the console is \nteamuser1\n and \nopenshift\n respectively.\n\n\n\n\n\n\n\n\n\n\nOnce you are into OpenShift Container Platform Console, click on your project \njupyterhub\n\n\n\n\n\n\n\n\nThe running pods should look like this when JupyterHub is ready\n\n\n\n\n\n\n\n\nVerify you can access your JupyterHub application by visiting the application URL\n\n\n\n\n\n\n\n\n(optional) You can also get your JupyterHub application URL by executing the below command on OpenShift Master Node.\n\n\n\n\necho \nhttps://\n$(oc get route jupyterhub -o jsonpath={.spec.host})\n\n\n\n\n\n\nFinally, log into JupyterHub by using the user name and password \nuser1\n and \n79e4e0\n respectively.\n\n\n\n\n\n\nTroubleshooting the JupyterHub deployment\n#\n\n\nIssue - 1 : Database not properly installed\n#\n\n\nSometimes the JupyterHub deployment to OpenShift runs into race conditions, that could trigger \nRecreate Deployment\n\n\n\n\n\n\n\n\nThe jupyterhub-db pod never comes up and constantly restarts.  This is often due to the PostgreSQL database not coming up cleanly.  If this occurs, the easiest thing to do is delete the jupyterhub-db persisted volume and rerun the oc process command.\n\n\n\n\n\n\nDelete the persisted volume:\n\n\n\n\n\n\n\n\n\n\n\n\nRerun oc process command:\n\n\n\n\noc login -u teamuser1 -p openshift\n\n\n\n\noc process jupyterhub-ocp-oauth HASH_AUTHENTICATOR_SECRET_KEY=\nmeh\n | oc apply -f -\n\n\n\n\n\n\nWait for the pods to come up cleanly:\n\n\n\n\n\n\nIssue - 2 : 500 Internal Error when logging into JupyterHub\n#\n\n\n\n\nYou receive a \n500 Internal Error\n.  Do another deployment/rollout of JupyterHub which forces a restart of the server container and after connecting again to database.\n\n\n\n\noc rollout latest jupyterhub\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached to the end of Module-2. At this point you have learned how to deploy an application on OCP. In the later modules we will use this application to perform some interesting Data Analytics and Machine Learning tasks", 
            "title": "Module-2 : Setting up JupyterHub in OpenShift Container Platform"
        }, 
        {
            "location": "/Module-2/#module-2-setting-up-jupyterhub-in-openshift-container-platform", 
            "text": "Module Agenda   In this module you will be creating an OpenShift project for JupyterHub required for the rest of the sessions     From your workstation SSH into  OpenShift Master Node  with the user name  cloud-user  and  SSH Private Key   (Need Help..Learn how to Login)   chmod 400  path to ssh_key.pem \nssh -i  path to ssh_key.pem  cloud-user@ OpenShift Master Node SSH IP Address    Prerequisite   You must run all the commands logged in as user  cloud-user  on the  OpenShift Master Node  node, unless otherwise specified.", 
            "title": "Module - 2 : Setting up JupyterHub in OpenShift Container Platform"
        }, 
        {
            "location": "/Module-2/#create-an-openshift-project", 
            "text": "In order to save your precious lab time, OpenShift Container Platform has already been installed and configured. Before you begin with some data science exercises, let s create an OpenShift project.   SSH into OpenShift Master Node as  cloud-user   ssh -i  path to ssh_key.pem  cloud-user@ OpenShift Master Node IP Address    Login to OpenShift   oc login -u teamuser1 -p openshift   Create a new project   oc new-project jupyterhub   Prepare the OpenShift notebooks and templates   oc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/notebooks.json  oc apply -f https://raw.githubusercontent.com/vpavlin/jupyterhub-ocp-oauth/ceph-summit-demo/templates.json   Process the template to deploy the JupyterHub application   oc process jupyterhub-ocp-oauth HASH_AUTHENTICATOR_SECRET_KEY= meh  | oc apply -f -   Once templates are successfully applied, the output should look like this   imagestream  jupyterhub-img  created\nbuildconfig  jupyterhub-img  created\nconfigmap  jupyterhub-cfg  created\nserviceaccount  jupyterhub-hub  created\nrolebinding  jupyterhub-edit  created\ndeploymentconfig  jupyterhub  created\nservice  jupyterhub  created\nroute  jupyterhub  created\npersistentvolumeclaim  jupyterhub-db  created\ndeploymentconfig  jupyterhub-db  created\nservice  jupyterhub-db  created   You should now have JupyterHub pods and services coming up (will take some time to fully start).   To check the status of pods and services, execute   oc get pods   The deployment is complete when the jupyterhub-db and jupyter pods are fully running and the output should look like this.   $ oc get pods\nNAME                     READY     STATUS    RESTARTS   AGE\njupyterhub-db-1-deploy    1 /1       Running    0           19s\njupyterhub-db-1-zkmxc     0 /1       Running    0           17s\njupyterhub-img-1-build    1 /1       Running    0           20s    You could also monitor your application from OpenShift Container Platform Console by visiting  OpenShift Web Console URL  that you can get from  Qwiklab Portal under Connection Details .    The user name and password to access the console is  teamuser1  and  openshift  respectively.      Once you are into OpenShift Container Platform Console, click on your project  jupyterhub     The running pods should look like this when JupyterHub is ready     Verify you can access your JupyterHub application by visiting the application URL     (optional) You can also get your JupyterHub application URL by executing the below command on OpenShift Master Node.   echo  https:// $(oc get route jupyterhub -o jsonpath={.spec.host})   Finally, log into JupyterHub by using the user name and password  user1  and  79e4e0  respectively.", 
            "title": "Create an OpenShift Project"
        }, 
        {
            "location": "/Module-2/#troubleshooting-the-jupyterhub-deployment", 
            "text": "", 
            "title": "Troubleshooting the JupyterHub deployment"
        }, 
        {
            "location": "/Module-2/#issue-1-database-not-properly-installed", 
            "text": "Sometimes the JupyterHub deployment to OpenShift runs into race conditions, that could trigger  Recreate Deployment     The jupyterhub-db pod never comes up and constantly restarts.  This is often due to the PostgreSQL database not coming up cleanly.  If this occurs, the easiest thing to do is delete the jupyterhub-db persisted volume and rerun the oc process command.    Delete the persisted volume:       Rerun oc process command:   oc login -u teamuser1 -p openshift  oc process jupyterhub-ocp-oauth HASH_AUTHENTICATOR_SECRET_KEY= meh  | oc apply -f -   Wait for the pods to come up cleanly:", 
            "title": "Issue - 1 : Database not properly installed"
        }, 
        {
            "location": "/Module-2/#issue-2-500-internal-error-when-logging-into-jupyterhub", 
            "text": "You receive a  500 Internal Error .  Do another deployment/rollout of JupyterHub which forces a restart of the server container and after connecting again to database.   oc rollout latest jupyterhub   End of Module  We have reached to the end of Module-2. At this point you have learned how to deploy an application on OCP. In the later modules we will use this application to perform some interesting Data Analytics and Machine Learning tasks", 
            "title": "Issue - 2 : 500 Internal Error when logging into JupyterHub"
        }, 
        {
            "location": "/Module-3/", 
            "text": "Module - 3 : Introduction to S3A \n Loading Sample Data Set\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will be introduced to the S3A filesystem client\n\n\nLocally download sample data sets of metrics data and sample trip reports.\n\n\nCreate a bucket and ingest the local data set to your Ceph object store\n\n\n\n\n\n\n\n\nFrom your workstation, SSH into \nceph-node1\n node as user \nstudent\n and password \nRedhat18\n \n(Need Help..Learn how to Login)\n\n\n\n\nssh student@\nIP Address of ceph-node1 node\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou need to have completed Modules 1-2 before beginning this module\n\n\n\n\n\n\nIntroduction\n#\n\n\nThe S3A filesystem client comes from Apache Hadoop and can be utilized by Spark and other tools to interact with a S3 compatible object storage system. S3A is the most feature complete and robust client for interacting with S3 compatible object storage systems, successor to the S3N and S3 filesystem clients. While S3A makes every attempt to map the HDFS API closely to the S3 API, there are some semantic differences inherrent to object storage (Reference: Hadoop Wiki):\n\n\n\n\nEventual Consistency, with Amazon S3, or Multi-site Ceph. Creation, updates, deletes may not be visible for an undefined time.\n\n\nNon-atomic rename and delete operations. Renaming or deleting large directories takes time proportional to the number of entries and visible to other processes during this time, and indeed, until the eventual consistency has been resolved.\n\n\n\n\nEndpoints, Credentials, and SSL\n#\n\n\nThe default endpoint for S3A routes requests to Amazon S3 with SSL (TLS). When interacting with a Ceph object store you will need to change a few S3A configuration parameters, notably the endpoint should be the DNS name or IP address of your Ceph object storage API endpoint.\n\n\n\n\nEndpoint (\nfs.s3a.endpoint\n)\n\n\nSSL (\nfs.s3a.connection.ssl.enabled\n)\n\n\n\n\n\n\nTip\n\n\nIf HTTP(S) is included in the endpoint definition, then the SSL property is automatically adjusted as appropriate. We will use this convention in the later module when we update the Jupyter Notebook.\n\n\n\n\nLoading the sample data sets into Ceph object storage\n#\n\n\nTo load the data sets into your Ceph object store we will use S3cmd, a python CLI tool for interacting with S3 compatible object stores. For convenience S3cmd was pre-installed on \nceph-node1\n.\n\n\n\n\n\n\nIf you have not already done, SSH into \nceph-node1\n as \nstudent\n user and \nRedhat18\n password.\n\n\n\n\n\n\nLocally download the sample data sets on \nceph-node\n.\n\n\n\n\n\n\nwget -O /home/student/kubelet_docker_operations_latency_microseconds.zip https://s3.amazonaws.com/bd-dist/kubelet_docker_operations_latency_microseconds.zip\n\n\n\n\nwget -O /home/student/trip_report.tsv https://s3.amazonaws.com/bd-dist/trip_report.tsv\n\n\n\n\n\n\nUnzip the sample metrics data set\n\n\n\n\nunzip /home/student/kubelet_docker_operations_latency_microseconds.zip -d /home/student/METRICS\n\n\n\n\n\n\nTip\n\n\nThe S3 API provides two ways to route requests to a particular bucket. The first is to use a bucket domain name prefix, meaning the API request will be sent to whichever IP address the \n.\n hostname resolves to. If a wildcard DNS subdomain is not configured for the S3 endpoint, or if the endpoint domain name is not configured in Ceph, then requests using this convention will fail. The second way of routing requests is to use path style access, meaning the API request will be sent to \n/\n. We will create a bucket with all upper case letters, as this convention instructs the S3A client to use the latter, path style approach.\n\n\n\n\n\n\nConfigure \nS3cmd\n\n\n\n\ns3cmd --access_key=S3user1 --secret_key=S3user1key --no-ssl --host=ceph-admin --host-bucket=\n%(bucket)s.ceph-admin\n --dump-config \n /home/student/.s3cfg\n\n\n\n\n\n\nCreate bucket for loading the sample metrics data set into Ceph object store\n\n\n\n\ns3cmd mb s3://METRICS\n\n\n\n\nNext, we will use the \nsync\n S3cmd command to synchronize the local directory with the downloaded data set with the newly created bucket. This is roughly equivalent to using rsync between two filesystems.\n\n\n\n\nSync local directory with METRICS bucket\n\n\n\n\ns3cmd sync /home/student/METRICS/ s3://METRICS\n\n\n\n\n\n\nVerify that metrics dataset is safe and sound in Ceph Object Storage\n\n\n\n\ns3cmd ls s3://METRICS/kubelet_docker_operations_latency_microseconds/\n\n\n\n\n\n\nCreate bucket for loading the sample trip report data set into Ceph object store\n\n\n\n\ns3cmd mb s3://SENTIMENT\n\n\n\n\n\n\nCopy local trip report file to SENTIMENT bucket\n\n\n\n\ns3cmd put /home/student/trip_report.tsv s3://SENTIMENT/data/trip_report.tsv\n\n\n\n\n\n\nVerify that trip report dataset is safe and sound in Ceph Object Storage\n\n\n\n\ns3cmd ls s3://SENTIMENT/data/\n\n\n\n\n\n\nNow we have our sample data sets ready to be used in Ceph object store.\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-3. In this module, you downloaded the sample data sets and uploaded them to your Ceph object store using S3cmd. In the next module you will do some basic analysis on these data sets", 
            "title": "Module-3 : Introduction to S3A & Loading Sample Data Sets"
        }, 
        {
            "location": "/Module-3/#module-3-introduction-to-s3a-loading-sample-data-set", 
            "text": "Module Agenda   In this module, you will be introduced to the S3A filesystem client  Locally download sample data sets of metrics data and sample trip reports.  Create a bucket and ingest the local data set to your Ceph object store     From your workstation, SSH into  ceph-node1  node as user  student  and password  Redhat18   (Need Help..Learn how to Login)   ssh student@ IP Address of ceph-node1 node    Prerequisite   You need to have completed Modules 1-2 before beginning this module", 
            "title": "Module - 3 : Introduction to S3A &amp; Loading Sample Data Set"
        }, 
        {
            "location": "/Module-3/#introduction", 
            "text": "The S3A filesystem client comes from Apache Hadoop and can be utilized by Spark and other tools to interact with a S3 compatible object storage system. S3A is the most feature complete and robust client for interacting with S3 compatible object storage systems, successor to the S3N and S3 filesystem clients. While S3A makes every attempt to map the HDFS API closely to the S3 API, there are some semantic differences inherrent to object storage (Reference: Hadoop Wiki):   Eventual Consistency, with Amazon S3, or Multi-site Ceph. Creation, updates, deletes may not be visible for an undefined time.  Non-atomic rename and delete operations. Renaming or deleting large directories takes time proportional to the number of entries and visible to other processes during this time, and indeed, until the eventual consistency has been resolved.", 
            "title": "Introduction"
        }, 
        {
            "location": "/Module-3/#endpoints-credentials-and-ssl", 
            "text": "The default endpoint for S3A routes requests to Amazon S3 with SSL (TLS). When interacting with a Ceph object store you will need to change a few S3A configuration parameters, notably the endpoint should be the DNS name or IP address of your Ceph object storage API endpoint.   Endpoint ( fs.s3a.endpoint )  SSL ( fs.s3a.connection.ssl.enabled )    Tip  If HTTP(S) is included in the endpoint definition, then the SSL property is automatically adjusted as appropriate. We will use this convention in the later module when we update the Jupyter Notebook.", 
            "title": "Endpoints, Credentials, and SSL"
        }, 
        {
            "location": "/Module-3/#loading-the-sample-data-sets-into-ceph-object-storage", 
            "text": "To load the data sets into your Ceph object store we will use S3cmd, a python CLI tool for interacting with S3 compatible object stores. For convenience S3cmd was pre-installed on  ceph-node1 .    If you have not already done, SSH into  ceph-node1  as  student  user and  Redhat18  password.    Locally download the sample data sets on  ceph-node .    wget -O /home/student/kubelet_docker_operations_latency_microseconds.zip https://s3.amazonaws.com/bd-dist/kubelet_docker_operations_latency_microseconds.zip  wget -O /home/student/trip_report.tsv https://s3.amazonaws.com/bd-dist/trip_report.tsv   Unzip the sample metrics data set   unzip /home/student/kubelet_docker_operations_latency_microseconds.zip -d /home/student/METRICS   Tip  The S3 API provides two ways to route requests to a particular bucket. The first is to use a bucket domain name prefix, meaning the API request will be sent to whichever IP address the  .  hostname resolves to. If a wildcard DNS subdomain is not configured for the S3 endpoint, or if the endpoint domain name is not configured in Ceph, then requests using this convention will fail. The second way of routing requests is to use path style access, meaning the API request will be sent to  / . We will create a bucket with all upper case letters, as this convention instructs the S3A client to use the latter, path style approach.    Configure  S3cmd   s3cmd --access_key=S3user1 --secret_key=S3user1key --no-ssl --host=ceph-admin --host-bucket= %(bucket)s.ceph-admin  --dump-config   /home/student/.s3cfg   Create bucket for loading the sample metrics data set into Ceph object store   s3cmd mb s3://METRICS  Next, we will use the  sync  S3cmd command to synchronize the local directory with the downloaded data set with the newly created bucket. This is roughly equivalent to using rsync between two filesystems.   Sync local directory with METRICS bucket   s3cmd sync /home/student/METRICS/ s3://METRICS   Verify that metrics dataset is safe and sound in Ceph Object Storage   s3cmd ls s3://METRICS/kubelet_docker_operations_latency_microseconds/   Create bucket for loading the sample trip report data set into Ceph object store   s3cmd mb s3://SENTIMENT   Copy local trip report file to SENTIMENT bucket   s3cmd put /home/student/trip_report.tsv s3://SENTIMENT/data/trip_report.tsv   Verify that trip report dataset is safe and sound in Ceph Object Storage   s3cmd ls s3://SENTIMENT/data/   Now we have our sample data sets ready to be used in Ceph object store.    End of Module  We have reached the end of Module-3. In this module, you downloaded the sample data sets and uploaded them to your Ceph object store using S3cmd. In the next module you will do some basic analysis on these data sets", 
            "title": "Loading the sample data sets into Ceph object storage"
        }, 
        {
            "location": "/Module-4/", 
            "text": "Module - 4 : Analytics on Metrics Data\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will be doing basic analysis of a Metrics data set from Prometheus\n\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou need to have completed Modules 1-3 before beginning this module\n\n\n\n\n\n\n\n\n\n\nThe instructions for this excercise are available as Juypter Notebook (\n.ipynb\n) that you can download from \nhere\n (Right click \n Save Link As \nipynb\n)\n\n\n\n\n\n\nAn active JupyterHub instance is required to open this notebook. Use the JupyterHub application that you have deployed in module-2. \n\n\n\n\nLogin to the \nOpenShift Container Platform Console\n and click on the JupyterHub application endpoint URL from the Overview screen. \n\n\n\n\n\n\n\n\nUse the following credentials to login into the JupyterHub application\n\nUser Name : \nuser1\n\nPassword  : \n79e4e0\n  \n\n\n\n\n\n\n\n\nImportant\n\n\nIf JupyterHub did not deploy cleanly, refer to the troubleshooting steps in Module 2 to redeploy.\n\n\n\n\n\n\n\n\nOn Juypter, select \nStart My Server\n\n\n\n\n\n\nSelect the \nspark-scipy-notebook:latest\n notebook image\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSpawn\n and wait for your notebook server to start, this may take a minute\n\n\n\n\n\n\nOnce the notebook server has started, you will see a folder view of the notebook file system\n\n\n\n\n\n\nClick the \nUpload\n button to the right \n\n\n\n\n\n\n \n\n\n\n\n\n\nFind the \nCeph_Data_Show_Lab_1.ipynb\n notebook file you have downloaded at the start of this module and upload it\n\n\n\n\n\n\nClick on the \nUpload\n button to finish uploading the notebook to JupyterHub \n \n\n\n\n\n\n\nSelect the \nCeph_Data_Show_Lab_1.ipynb\n notebook to begin analyzing the data\n\n\n\n\n\n\n\n\n\n\nReview the section \nAccess Ceph Objbect Storage over S3A\n, the configuration here should look like this\n\n\n\n\n\n\n\n\nBefore running any of the cells in the notebook, select the first cell (the beginning of the notebook).  Once the first cell is selected, click the \nRun\n button in the toolbar on each cell, stepping through the notebook and its results  \n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-4. In this module, you downloaded a sample Jupyter notebook exercise, uploaded it to JupyterHub and used it to analyze Prometheus data stored in Ceph. In the next module you will use machine learning libraries to train a model to detect the sentiment of a customer trip report.", 
            "title": "Module-4 : Analytics on Metrics Data"
        }, 
        {
            "location": "/Module-4/#module-4-analytics-on-metrics-data", 
            "text": "Module Agenda   In this module, you will be doing basic analysis of a Metrics data set from Prometheus     Prerequisite   You need to have completed Modules 1-3 before beginning this module      The instructions for this excercise are available as Juypter Notebook ( .ipynb ) that you can download from  here  (Right click   Save Link As  ipynb )    An active JupyterHub instance is required to open this notebook. Use the JupyterHub application that you have deployed in module-2.    Login to the  OpenShift Container Platform Console  and click on the JupyterHub application endpoint URL from the Overview screen.      Use the following credentials to login into the JupyterHub application \nUser Name :  user1 \nPassword  :  79e4e0        Important  If JupyterHub did not deploy cleanly, refer to the troubleshooting steps in Module 2 to redeploy.     On Juypter, select  Start My Server    Select the  spark-scipy-notebook:latest  notebook image       Click  Spawn  and wait for your notebook server to start, this may take a minute    Once the notebook server has started, you will see a folder view of the notebook file system    Click the  Upload  button to the right          Find the  Ceph_Data_Show_Lab_1.ipynb  notebook file you have downloaded at the start of this module and upload it    Click on the  Upload  button to finish uploading the notebook to JupyterHub       Select the  Ceph_Data_Show_Lab_1.ipynb  notebook to begin analyzing the data      Review the section  Access Ceph Objbect Storage over S3A , the configuration here should look like this     Before running any of the cells in the notebook, select the first cell (the beginning of the notebook).  Once the first cell is selected, click the  Run  button in the toolbar on each cell, stepping through the notebook and its results      End of Module  We have reached the end of Module-4. In this module, you downloaded a sample Jupyter notebook exercise, uploaded it to JupyterHub and used it to analyze Prometheus data stored in Ceph. In the next module you will use machine learning libraries to train a model to detect the sentiment of a customer trip report.", 
            "title": "Module - 4 : Analytics on Metrics Data"
        }, 
        {
            "location": "/Module-5/", 
            "text": "Module - 5 : ML using Jupyter Notebooks on Ceph\n#\n\n\n\n\nModule Agenda\n\n\n\n\nIn this module, you will be creating a model using data stored in Ceph to detect the sentiment of customer trip reports, and uploading the trained model back to Ceph\n\n\n\n\n\n\n\n\nPrerequisite\n\n\n\n\nYou need to have completed Modules 1-3 before beginning this module\n\n\n\n\n\n\n\n\n\n\nThe instructions for this excercise are available as Juypter Notebook (\n.ipynb\n) that you can download from \nhere\n (Right click \n Save Link As \nipynb\n)\n\n\n\n\n\n\nAn active JupyterHub instance is required to open this notebook. Use the JupyterHub application that you have deployed in module-2. \n\n\n\n\nLogin to the \nOpenShift Container Platform Console\n and click on the JupyterHub application endpoint URL from the Overview screen. \n\n\n\n\n\n\n\n\nUse the following credentials to login into the JupyterHub application\n\nUser Name : \nuser1\n\nPassword  : \n79e4e0\n  \n\n\n\n\n\n\n\n\nImportant\n\n\nIf JupyterHub did not deploy cleanly, refer to the troubleshooting steps in Module 2 to redeploy.\n\n\n\n\n\n\nClick the \nUpload\n button to the right \n\n\n\n\n \n\n\n\n\n\n\nFind the \nCeph_Data_Show_Lab_2.ipynb\n downloaded at the start of this module and upload it to JupyterHub\n\n\n\n\n\n\nClick on the \nUpload\n button to finish uploading the notebook to JupyterHub \n \n\n\n\n\n\n\nSelect the \nCeph_Data_Show_Lab_2.ipynb\n notebook to begin building a model for machine learning\n\n\n\n\n\n\nReview the section \nAccess Ceph Objbect Storage over S3A\n, the configuration here should look like this\n\n\n\n\n\n\n\n\n\n\nBefore running any of the cells in the notebook, select the first cell (the beginning of the notebook). Once the first cell is selected, click the \nRun\n button in the toolbar on each cell, stepping through the notebook and its results \n\n\n\n\n\n\n\n\nEnd of Module\n\n\nWe have reached the end of Module-5. In this module, you created a machine learning model using data stored in Ceph and uploaded the model to Ceph for future use.", 
            "title": "Module-5 : ML using Jupyter Notebooks on Ceph"
        }, 
        {
            "location": "/Module-5/#module-5-ml-using-jupyter-notebooks-on-ceph", 
            "text": "Module Agenda   In this module, you will be creating a model using data stored in Ceph to detect the sentiment of customer trip reports, and uploading the trained model back to Ceph     Prerequisite   You need to have completed Modules 1-3 before beginning this module      The instructions for this excercise are available as Juypter Notebook ( .ipynb ) that you can download from  here  (Right click   Save Link As  ipynb )    An active JupyterHub instance is required to open this notebook. Use the JupyterHub application that you have deployed in module-2.    Login to the  OpenShift Container Platform Console  and click on the JupyterHub application endpoint URL from the Overview screen.      Use the following credentials to login into the JupyterHub application \nUser Name :  user1 \nPassword  :  79e4e0        Important  If JupyterHub did not deploy cleanly, refer to the troubleshooting steps in Module 2 to redeploy.    Click the  Upload  button to the right         Find the  Ceph_Data_Show_Lab_2.ipynb  downloaded at the start of this module and upload it to JupyterHub    Click on the  Upload  button to finish uploading the notebook to JupyterHub       Select the  Ceph_Data_Show_Lab_2.ipynb  notebook to begin building a model for machine learning    Review the section  Access Ceph Objbect Storage over S3A , the configuration here should look like this      Before running any of the cells in the notebook, select the first cell (the beginning of the notebook). Once the first cell is selected, click the  Run  button in the toolbar on each cell, stepping through the notebook and its results      End of Module  We have reached the end of Module-5. In this module, you created a machine learning model using data stored in Ceph and uploaded the model to Ceph for future use.", 
            "title": "Module - 5 : ML using Jupyter Notebooks on Ceph"
        }
    ]
}